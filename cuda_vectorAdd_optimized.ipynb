{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elKRb_DGr7Q3",
        "outputId": "f8d9cbbb-7020-489a-9307-9e9f437026d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_grid.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "    int N = 1 << 20;  // 2^20 = 1M elements\n",
        "    float *x, *y;\n",
        "    cudaError_t err; // Declare error variable\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaDeviceProp prop;\n",
        "\n",
        "    // Allocate Unified Memory â€“ accessible from CPU or GPU\n",
        "    err = cudaMallocManaged(&x, N * sizeof(float));\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"cudaMallocManaged for x failed: \" << cudaGetErrorString(err) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "    err = cudaMallocManaged(&y, N * sizeof(float));\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"cudaMallocManaged for y failed: \" << cudaGetErrorString(err) << std::endl;\n",
        "        // Free x if it was allocated\n",
        "        cudaFree(x);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Initialize input arrays\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Launch the kernel\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "\n",
        "    float ms = 0;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"add kernel launch failed: \" << cudaGetErrorString(err) << std::endl;\n",
        "        // Free memory\n",
        "        cudaFree(x);\n",
        "        cudaFree(y);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Check for errors during device synchronization\n",
        "    err = cudaGetLastError();\n",
        "     if (err != cudaSuccess) {\n",
        "        std::cerr << \"cudaDeviceSynchronize failed: \" << cudaGetErrorString(err) << std::endl;\n",
        "        // Free memory\n",
        "        cudaFree(x);\n",
        "        cudaFree(y);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Verify result\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
        "    }\n",
        "\n",
        "    std::cout << \" Max error: \" << maxError << std::endl;\n",
        "\n",
        "    // Print simulated profile\n",
        "    std::cout << \"Kernel time: \" << ms << \" ms\" << std::endl;\n",
        "    std::cout << \"Array size: \" << N << std::endl;\n",
        "\n",
        "    std::cout << \"GPU: \" << prop.name << \"\\n\";\n",
        "    std::cout << \"SMs: \" << prop.multiProcessorCount << \"\\n\";\n",
        "    std::cout << \"Shared memory per block: \" << prop.sharedMemPerBlock << \" bytes\\n\";\n",
        "    std::cout << \"Max threads per block: \" << prop.maxThreadsPerBlock << \"\\n\";\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -arch=sm_70 -Xptxas -v add_grid.cu -o add_grid 2> compile_log.txt\n",
        "./add_grid > runtime_log.txt 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlPuo7fYsbCr",
        "outputId": "0d8bc3a0-5067-4b6b-87e7-2a4f32db52de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat add_grid.cu compile_log.txt runtime_log.txt > cuwise_input.txt"
      ],
      "metadata": {
        "id": "oNihe0zOO1pC"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}